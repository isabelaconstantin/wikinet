{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import wikipedia\n",
    "import datetime\n",
    "import requests\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import urllib\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "from collections import deque\n",
    "# documentation: https://mwparserfromhell.readthedocs.io/en/latest/api/mwparserfromhell.nodes.html#module-mwparserfromhell.nodes.wikilink\n",
    "import mwparserfromhell\n",
    "import networkx as nx\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserActivity(article, granularity, start, end, project =\"en.wikipedia.org\",\n",
    "                    access=\"all-access\", agent=\"user\",dateformat=\"iso\"):\n",
    "    \"\"\"\n",
    "    Method to obtain user activity of a given page for a given period of time\n",
    "    article: name of the wikiipedia article\n",
    "    granularity: time granularity of activity, either 'monthly' or 'daily'\n",
    "    start: start date of the research as Datetime.datetime object\n",
    "    end: end date of the research as Datetime.datetime object\n",
    "    project: If you want to filter by project, use the domain of any Wikimedia project (by default en.wikipedia.org)\n",
    "    access: If you want to filter by access method, use one of desktop, mobile-app or mobile-web (by default all-access)\n",
    "    agent: If you want to filter by agent type, use one of user, bot or spider (by default user).\n",
    "    dateformat: the dateformat used in result array, can be 'iso','ordinal','datetime'.\n",
    "    return:\n",
    "        it return an array of array of the form [ [user_activity_value1, date1], [user_activity_value2, date2]]\n",
    "    \"\"\"\n",
    "\n",
    "    #granularity['monthly','daily']\n",
    "    #format['iso','ordinal','datetime']\n",
    "    #Be carefull, for daily granularity left bound date is included, for monthly granularity left bound date is excluded\n",
    "    \n",
    "    dstart = start.strftime(\"%Y%m%d\")\n",
    "    dend = end.strftime(\"%Y%m%d\")\n",
    "    path = (\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"+project\n",
    "            +\"/\"+access+\"/\"+agent+\"/\"+article+\"/\"+granularity+\"/\"+dstart+\"/\"+dend)\n",
    "    r = requests.get(path)\n",
    "    if not r.ok:\n",
    "        print('Request Error: ', r)\n",
    "        return\n",
    "    res = []\n",
    "    for i in range(len(r.json()['items'])):\n",
    "        time_label = None\n",
    "        if granularity == 'daily':\n",
    "            time_label = (start + datetime.timedelta(days=i))\n",
    "        else:\n",
    "            time_label = (start + relativedelta(months=+i))\n",
    "        if dateformat == 'iso':\n",
    "            time_label = time_label.isoformat()\n",
    "        elif dateformat == 'ordinal':\n",
    "            time_label = time_label.toordinal()\n",
    "            \n",
    "        res.append([r.json()['items'][i]['views'],time_label])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "# TODO: make it deal with server side error \n",
    "def parse_with_date(title, date=None):\n",
    "    '''\n",
    "    \n",
    "    :param title: title of the wikipedia article\n",
    "    :param date: snapshot of the page as it was on the date. If None, scrapes the page as is now. \n",
    "    :return: the parsed page of wikipedia\n",
    "    '''\n",
    "    data = {\"action\": \"query\", \"prop\": \"revisions\", \"rvlimit\": 1,\n",
    "            \"rvprop\": \"content\", \"format\": \"json\", \"titles\": title}\n",
    "    if date is not None:\n",
    "        data[\"rvstart\"] = date\n",
    "    raw = urlopen(API_URL, urlencode(data).encode()).read()\n",
    "    res = json.loads(raw)\n",
    "    try:\n",
    "        text = list(res[\"query\"][\"pages\"].values())[0][\"revisions\"][0][\"*\"]\n",
    "    except KeyError as err:\n",
    "        print(\"Key error\".format(err))\n",
    "        print(title)\n",
    "        return None\n",
    "    return mwparserfromhell.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'Stan Lee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_lee_12_nov = parse_with_date(seed, date= '2018-11-13T00:00:00Z')\n",
    "stan_lee_12_nov_l = stan_lee_12_nov.filter_wikilinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,\n",
       " ['[[New York City]]',\n",
       "  '[[Los Angeles]]',\n",
       "  '[[California]]',\n",
       "  '[[Jack Kirby]]',\n",
       "  '[[Steve Ditko]]',\n",
       "  '[[John Romita Sr.]]',\n",
       "  '[[Don Heck]]',\n",
       "  '[[Bill Everett]]',\n",
       "  '[[Joe Maneely]]',\n",
       "  '[[The Will Eisner Award Hall of Fame]]'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st hop links. \n",
    "len(stan_lee_12_nov_l), stan_lee_12_nov_l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_links(links):\n",
    "    non_link = re.compile('Category:|File:|wikt:|.*#.*')\n",
    "    links = [str(link.title) for link in links if non_link.match(str(link.title))==None]\n",
    "    links = list(set(links))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = preprocess_links(stan_lee_12_nov_l)\n",
    "len(links), links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links), type(stan_lee_12_nov_l[0].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "A heuristic we could use for sampling is to give more probability to the links at the front of the page (assuming they might be more important and more related to the subject) than to the other links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_links(n_links, n_links_sample, percentage_l, percentage_l_subsample):\n",
    "    '''\n",
    "    Example : make the first 20% of the links to account for 50% of the subsample\n",
    "    :param n_links: number of links a page has\n",
    "    :param n_links_sample: number of links you want to sample\n",
    "    :param percentage_l: consider the top percentage_l links ..\n",
    "    :param percentage_l_subsample: to account for percentage_l_subsample of the subsampled links\n",
    "    :return:  the chosen links to select\n",
    "    NOTE: if percentage_l = percentage_l_subsample => aprox uniform sampling \n",
    "    '''\n",
    "    # no sampling needed here\n",
    "    if n_links_sample >= n_links:\n",
    "        return np.array(range(n_links))\n",
    "    \n",
    "    # how many links from the first group should be subsampled\n",
    "    n_links_first_group = int(n_links_sample * percentage_l_subsample)\n",
    "    # how many links from the first group we have\n",
    "    n_links_first_group_pop = int(n_links * percentage_l)\n",
    "    \n",
    "    # no sampling from the first group\n",
    "    if n_links_first_group_pop < n_links_first_group:\n",
    "        links_chosen_group_1 = np.array(range(n_links_first_group_pop))\n",
    "        remaining = n_links_sample - n_links_first_group_pop\n",
    "        links_chosen_group_2 = np.random.choice(range(n_links_first_group_pop+1, n_links), size = remaining, replace= False)\n",
    "        return np.append(links_chosen_group_1, links_chosen_group_2)\n",
    "    \n",
    "    # if we have to sample from both groups, create the probabilities \n",
    "    perc_1 = percentage_l_subsample / n_links_first_group_pop\n",
    "    perc_2 = (1 - percentage_l_subsample) / (n_links - n_links_first_group_pop)\n",
    "    #print('Links from first group were sampled with p ', perc_1, ' and links from second group were sampled with p ', perc_2)\n",
    "    p = [perc_1] * n_links_first_group_pop + [perc_2] * (n_links - n_links_first_group_pop)\n",
    "    chosen_links = np.random.choice(n_links, size = n_links_sample, p = p, replace=False)\n",
    "    return chosen_links\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_links(100, 10, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_links(100, 10, 0.09, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 10 links should account for 50% of the subsampled links\n",
    "sample_links(100, 10, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling algorithm (BFS-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'Stan Lee'\n",
    "date = '2018-11-13T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'Rosetta (spacecraft)'\n",
    "date = '2014-11-12T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_crawl(seed, date):\n",
    "    # CONSTANTS: to be explored\n",
    "    max_nodes = 500\n",
    "    n_links_hop1 = 175 # how many direct neighbours\n",
    "    n_links_indirect_min = 10\n",
    "    n_links_indirect_max = 25\n",
    "    min_popularity = 30 # the minimum number of links a page should have to be taken into the graph (to avoid stubs or really small articles)\n",
    "\n",
    "    # init algo\n",
    "    article_links = {}\n",
    "    queue = deque([seed])\n",
    "\n",
    "    while(len(article_links) < max_nodes):\n",
    "        if len(queue) == 0:\n",
    "            print('no more links to dequeue')\n",
    "            break\n",
    "        article = queue.popleft()\n",
    "        # crawl it\n",
    "        article_content = parse_with_date(title=article, date=date)\n",
    "        # check parse succesfull\n",
    "        if article_content is not None:\n",
    "            links = article_content.filter_wikilinks()\n",
    "            links = preprocess_links(links)\n",
    "            #print(len(links))\n",
    "            if (len(links) > min_popularity):\n",
    "                article_links[article] = links\n",
    "                #print('Added ', article, ' to the graph')\n",
    "                if article == seed:\n",
    "                    idx_chosen_links = sample_links(n_links=len(links), n_links_sample=n_links_hop1, percentage_l=0.1, percentage_l_subsample=0.5)\n",
    "                else:\n",
    "                    '''\n",
    "                    if len(links) > 100:\n",
    "                        n_links_s = 50 # this is a hub, so get more links\n",
    "                    else:\n",
    "                        n_links_s = random.randint(n_links_indirect_min, n_links_indirect_max)\n",
    "                    '''\n",
    "                    n_links_s = random.randint(n_links_indirect_min, n_links_indirect_max)\n",
    "                    idx_chosen_links = sample_links(n_links=len(links), n_links_sample=n_links_s, percentage_l=0.4, percentage_l_subsample=0.5 )\n",
    "                for idx in idx_chosen_links:\n",
    "                    if links[idx] not in article_links and links[idx] not in queue:\n",
    "                        queue.append(links[idx])\n",
    "    return article_links\n",
    "\n",
    "links = wiki_crawl(seed, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(links):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(links.keys())\n",
    "    for article in G.nodes():\n",
    "        for link in links[article]:\n",
    "            # add link if not self-loop\n",
    "            if link in links and link != article:\n",
    "                G.add_edge(article, link)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = construct_graph(links)\n",
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the graph (take around half h to make)\n",
    "#nx.write_gpickle(G, \"graph2.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it was saved\n",
    "G2 = nx.read_gpickle(\"graph2.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2.number_of_edges(), G2.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G2.nodes() == G.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the delta page_view signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "#start = datetime.datetime(year=2018,month=11,day=11, hour=23, minute = 59, second = 59)\n",
    "#end =  datetime.datetime(year=2018,month=11,day=12, hour=23, minute = 59, second = 59)\n",
    "#daily_view = getUserActivity(article=\"Stan Lee\",granularity=\"daily\",start=start,end=end,dateformat=\"iso\")\n",
    "start = datetime.datetime(year=2014,month=11,day=12, hour=23, minute = 59, second = 59)\n",
    "end =  datetime.datetime(year=2014,month=11,day=11, hour=23, minute = 59, second = 59)\n",
    "daily_view = getUserActivity(article='Rosetta (spacecraft)',granularity=\"daily\",start=start,end=end,dateformat=\"iso\")\n",
    "daily_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page views are crawled at the end of the day ! \n",
    "views = {}\n",
    "nodes_not_taken = []\n",
    "for node in G2.nodes():\n",
    "    try:\n",
    "        views[node] = getUserActivity(article=node, granularity=\"daily\",start=start,end=end,dateformat=\"iso\")\n",
    "    except KeyError:\n",
    "        nodes_not_taken.append(node)\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_graph = {}\n",
    "for node in G2.nodes():\n",
    "    if node in views.keys():\n",
    "        try:\n",
    "            views_graph[node] = views[node][1][0] -  views[node][0][0] \n",
    "        except IndexError:\n",
    "            views_graph[node] = views[node][0][0]\n",
    "    else:\n",
    "        views_graph[node] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(n, views_graph[n]) for n in list(views_graph.keys())[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(views_graph,dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G2, values= views_graph, name = 'delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "G2.node['Stan Lee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save again \n",
    "nx.write_gpickle(G2, \"graph_with_delta_signal.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
